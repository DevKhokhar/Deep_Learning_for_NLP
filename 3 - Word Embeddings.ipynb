{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing the meaning of a word the linguistic way  \n",
    "\n",
    "#### Definition of meaning (from Webster dictionary)  \n",
    "* the idea that is represented by a word, phrase, etc.  \n",
    "* the idea that a person wants to express by using words, signs, etc.  \n",
    "* the idea that is expressed in a work of writing, art, etc.  \n",
    "\n",
    "Linguistic way of defining meaning : denotation (signifier)\n",
    "\n",
    "__Source__:  \n",
    "[Deep Learning for Natural Language Processing, Stanford University School of Engineering](https://www.youtube.com/watch?v=ERibwqs9p38&list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6&index=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is WordNet?  \n",
    "WordNet is a dataset that uses taxonomy for words and has hypernyms (is-a) relationships and synonyms sets. It can be accessed via the nltk.corpus library (see code below for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pip\n",
    "import sys\n",
    "if not 'nltk' in sys.modules.keys():\n",
    "    pip.main(['install', 'nltk'])\n",
    "    import nltk\n",
    "    nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('primate.n.02'),\n",
       " Synset('placental.n.01'),\n",
       " Synset('mammal.n.01'),\n",
       " Synset('vertebrate.n.01'),\n",
       " Synset('chordate.n.01'),\n",
       " Synset('animal.n.01'),\n",
       " Synset('organism.n.01'),\n",
       " Synset('living_thing.n.01'),\n",
       " Synset('whole.n.02'),\n",
       " Synset('object.n.01'),\n",
       " Synset('physical_entity.n.01'),\n",
       " Synset('entity.n.01')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "monkey = wn.synset('monkey.n.01')\n",
    "hyper = lambda s: s.hypernyms()\n",
    "list(monkey.closure(hyper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with discrete representation such as that in WordNet  \n",
    "* Also called as local representation\n",
    "* Great as a resource but missing nuances. For e.g., synonyms\n",
    "* Missing new words\n",
    "* Subjective\n",
    "* Requires human labour to create and adapt\n",
    "* Hard to compute accurate word similarity\n",
    "* The vast majority of rule-based and statistical NLP work regards words as atomic symbols\n",
    "* In vector-space terms, this is a vector with one 1 and a lot of zeroes *(known as one-hot encoding/representation)*\n",
    "* In this case, the two vectors to be looked at for similarity are orthogonal. There is no natural notion of similarity in a set of one-hot vectors.\n",
    "\n",
    "__Source__:  \n",
    "[Deep Learning for Natural Language Processing, Stanford University School of Engineering](https://www.youtube.com/watch?v=ERibwqs9p38&list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6&index=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributional similarity is a solution to the above problems  \n",
    "* Represent a word by meaning of its neighbouring words. \n",
    "* This helps us build a dense vector for each word type, chosen so that it is good at predicting other words appearing in the context.  \n",
    "\n",
    "__Source__:  \n",
    "[Deep Learning for Natural Language Processing, Stanford University School of Engineering](https://www.youtube.com/watch?v=ERibwqs9p38&list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6&index=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Word Embeddings  \n",
    "We define a model that aims to predict between a center word w<sub>t</sub> and context words in terms of word vectors   \n",
    "> p(context|w<sub>t</sub>) = ...  \n",
    "\n",
    "which has a loss function, e.g.,\n",
    "> J = 1 - p(w<sub>-t</sub>|w<sub>t</sub>)\n",
    "\n",
    "We look at many positions t in a big language corpus. We keep adjusting the vector representations of words to minimize this loss.\n",
    "\n",
    "__Source__:  \n",
    "[Deep Learning for Natural Language Processing, Stanford University School of Engineering](https://www.youtube.com/watch?v=ERibwqs9p38&list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6&index=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References  \n",
    "* [DE Rumelhart (1986), Learning representations by back-propagating errors](https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf)  \n",
    "* [Bengio et al (2003), A neural probabilistic language model](www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    "* [R Collobert (2011), Natural Language Processing(almost) from Scratch](www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf)\n",
    "* [Mikolov et al (2013), Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
