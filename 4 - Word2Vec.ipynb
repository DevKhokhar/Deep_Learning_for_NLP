{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of Word2Vec  \n",
    "Predict between every word and its context words.  \n",
    "\n",
    "Two algorithms:  \n",
    "* __Skip-grams (SG) __: Predict context words given target(position independent)\n",
    "* __Continuous bag of words (CBOW) __: Predict target words from bag-of-words context  \n",
    "\n",
    "Two moderately efficient training methods:\n",
    "* Hierarchical softmax\n",
    "* Negative sampling \n",
    "\n",
    "__Source__:  \n",
    "[Deep Learning for Natural Language Processing, Stanford University School of Engineering](https://www.youtube.com/watch?v=ERibwqs9p38&list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6&index=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details of word2vec  \n",
    "\n",
    "For each word t=1...T, predict surrounding words in a window of radius m of every word.  \n",
    "\n",
    "__Objective function__: Maximize the probability of any context word given the current center word.  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;__J<sup>'</sup>(&theta;) = <sup>T</sup>&Pi;<sub>t=1</sub> &Pi;<sub>-m<=j<=m; j!=0</sub> p(w<sub>t+j</sub> | w<sub>t</sub>; &theta;)__\n",
    "\n",
    "__Negative log likelihood__  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;__J(&theta;) = (-1/T) <sup>T</sup>&Sigma;<sub>t=1</sub> &Sigma;<sub>-m<=j<=m; j!=0</sub> log p(w<sub>t+j</sub> | w<sub>t</sub>)  __\n",
    "\n",
    "where &theta; represents all variables we will optimize.  \n",
    "\n",
    "Predict surrounding words in a window of radius m of every word  \n",
    "\n",
    "For __p(w<sub>t+j</sub>|w<sub>t</sub>)__ the simplest first formulation is  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;__p(o|c) = exp(u<sub>o</sub><sup>T</sup>v<sub>c</sub>) / <sup>V</sup>&Sigma;<sub>w=1</sub> exp(u<sub>w</sub><sup>T</sup>v<sub>c</sub>)__  \n",
    "\n",
    "where o is the outside (or output) word index, c is the center word index, v<sub>c</sub> and u<sub>o</sub> are \"center\" and \"outside\" vectors of indices c and o.\n",
    "\n",
    "Softmax using word c to obtain probability of word o\n",
    "\n",
    "*[To-do] Elaborate on SoftMax and how did we arrive at the equation above.*\n",
    "\n",
    "__Source__:  \n",
    "[Deep Learning for Natural Language Processing, Stanford University School of Engineering](https://www.youtube.com/watch?v=ERibwqs9p38&list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6&index=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
