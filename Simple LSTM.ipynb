{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling using LSTM in keras/tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. \n",
    "\n",
    "We use tensorflow as the backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LSTM network and generate text\n",
    "import sys\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the following cell, make sure you have the file wonderland.txt in data folder in your working directory.\n",
    "\n",
    "The following snippet, generates the data by splitting the raw text into a list of characters. A vocabulary **chars** is generated using the characters present in the text. Since, the model can take only numbers as input, the characters are mapped to numbers, and a reverse mapping is created inorder to map the output generated by the model back to characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  163815\n",
      "Total Vocab:  60\n"
     ]
    }
   ],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"data/wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers, and a reverse mapping\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is split into sets of 100 characters for training and the character following the 100 characters as output. For example, a single set of data would look like this:\n",
    "\n",
    "seq_in : project gutenberg’s alice’s adventures in wonderland, by lewis carroll\n",
    "\n",
    "this ebook is for the use of\n",
    "\n",
    "seq_out : a\n",
    "\n",
    "All the input sequences(seq_in) are appended to dataX and output sequences(seq_out) to dataY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  163715\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is then prepared to be fed into the model, and the LSTM model is defined with 1 LSTM layer, 1 dropout layer and a Dense layer with softmax activation.\n",
    "\n",
    "Dropout layer : A dropout layer randomly ignores a certain percentage of units(neurons) while training, i.e their weights do not get updated\n",
    "\n",
    "Dense layer : A fully connected layer\n",
    "\n",
    "Training is carried out for 2 epochs with a batch size of 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "163715/163715 [==============================] - ETA: 0s - loss: 2.998 - 870s 5ms/step - loss: 2.9984\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.99837, saving model to weights/weights-improvement-01-2.9984.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x137809e80>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# define the checkpoint\n",
    "filepath=\"weights/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "graph_vis = TensorBoard(log_dir='./logs')\n",
    "callbacks_list = [checkpoint,graph_vis]\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=1, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While training, the weights are saved after each epoch as hdf5 files. You can delete all except the one with the least loss, which can be used later for testing. Once the training is done, we can test the model on a randomly chosen sequence of words from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" , two!’ said seven.\n",
      "\n",
      "‘yes, it is his business!’ said five, ‘and i’ll tell him--it was for\n",
      "bringing t \"\n",
      "oe tor toet toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tore toe tor\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
